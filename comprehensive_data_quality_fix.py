"""
COMPREHENSIVE DATA QUALITY FIX & PREVENTION SYSTEM

Root Causes Identified:
1. No unique constraints on business keys ‚Üí allows duplicate inserts
2. No pre-load validation ‚Üí duplicates not detected before insert
3. No post-load validation ‚Üí row count mismatches not caught
4. Loader truncate() allows re-upload without checking existing data

Prevention Strategy:
1. Add database UNIQUE constraints on business key combinations
2. Add pre-load duplicate detection in loaders
3. Add post-load row count validation (Excel vs DB)
4. Add automated data quality tests

Execution Plan:
Step 1: Re-load all raw tables from Excel (clean slate)
Step 2: Add UNIQUE constraints to prevent future duplicates
Step 3: Update loaders with validation logic
Step 4: Create automated quality tests
"""

import asyncio
from sqlalchemy import create_engine, text, UniqueConstraint, Index
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv
import sys
import pandas as pd
from pathlib import Path

load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)

# Excel file paths
BASE_DIR = Path(__file__).parent
DEMODATA_DIR = BASE_DIR / "demodata"

def print_section(title):
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80)

async def step1_reload_raw_data():
    """
    Step 1: Re-load ALL raw tables from Excel files
    Uses existing ETL loaders with truncate
    """
    print_section("STEP 1: RE-LOADING RAW DATA FROM EXCEL")
    
    sys.path.insert(0, 'src')
    from src.etl.loaders import load_all_raw_data
    
    session = Session()
    try:
        print("üîÑ Loading from Excel files...")
        results = load_all_raw_data(session)
        
        # Show results
        print("\nüìä LOAD RESULTS:")
        total_loaded = 0
        total_errors = 0
        for table, stats in results.items():
            loaded = stats.get('loaded', 0)
            errors = stats.get('errors', 0)
            total_loaded += loaded
            total_errors += errors
            
            status = "‚úì" if errors == 0 else "‚ö†"
            print(f"  {status} {table}: {loaded:,} rows loaded, {errors:,} errors")
        
        print(f"\nüìà TOTAL: {total_loaded:,} rows loaded, {total_errors:,} errors")
        
        # Validate against Excel files
        print("\nüîç VALIDATING AGAINST EXCEL FILES:")
        excel_files = {
            'cooispi': DEMODATA_DIR / 'cooispi.XLSX',
            'mb51': DEMODATA_DIR / 'mb51.XLSX',
            'zrmm024': DEMODATA_DIR / 'zrmm024.XLSX',
            'zrsd002': DEMODATA_DIR / 'zrsd002.xlsx',
            'zrsd004': DEMODATA_DIR / 'zrsd004.XLSX',
            'zrsd006': DEMODATA_DIR / 'zrsd006.XLSX',
            'zrfi005': DEMODATA_DIR / 'ZRFI005.XLSX',
        }
        
        table_map = {
            'cooispi': 'raw_cooispi',
            'mb51': 'raw_mb51',
            'zrmm024': 'raw_zrmm024',
            'zrsd002': 'raw_zrsd002',
            'zrsd004': 'raw_zrsd004',
            'zrsd006': 'raw_zrsd006',
            'zrfi005': 'raw_zrfi005',
        }
        
        validation_passed = True
        for name, excel_path in excel_files.items():
            if not excel_path.exists():
                continue
                
            df = pd.read_excel(excel_path)
            excel_count = len(df)
            
            table_name = table_map[name]
            db_count = session.execute(text(f"SELECT COUNT(*) FROM {table_name}")).scalar()
            
            diff_pct = abs(excel_count - db_count) / excel_count * 100 if excel_count > 0 else 0
            
            if diff_pct > 1:
                print(f"  ‚ö† {name}: Excel {excel_count:,} vs DB {db_count:,} ({diff_pct:.1f}% diff)")
                validation_passed = False
            else:
                print(f"  ‚úì {name}: {db_count:,} rows (matches Excel)")
        
        if validation_passed:
            print("\n‚úÖ All tables validated successfully!")
        else:
            print("\n‚ö†Ô∏è Some tables have mismatches - needs investigation")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        session.close()

async def step2_retransform_fact_tables():
    """
    Step 2: Re-transform fact tables from clean raw data
    """
    print_section("STEP 2: RE-TRANSFORMING FACT TABLES")
    
    sys.path.insert(0, 'src')
    from src.etl.transform import Transformer
    
    session = Session()
    try:
        transformer = Transformer(session)
        
        print("üîÑ Transforming raw ‚Üí fact tables...")
        transformer.transform_all()
        
        # Validate fact tables
        print("\nüìä FACT TABLE COUNTS:")
        fact_tables = [
            'fact_production',
            'fact_billing',
            'fact_delivery',
            'fact_ar_aging',
            'fact_lead_time',
            'fact_alerts'
        ]
        
        for table in fact_tables:
            count = session.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
            print(f"  {table}: {count:,} rows")
        
        print("\n‚úÖ Transform completed!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        session.close()

async def step3_add_unique_constraints():
    """
    Step 3: Add UNIQUE constraints on business keys to PREVENT duplicates at DB level
    
    This is the most important prevention mechanism - database will reject duplicate inserts
    """
    print_section("STEP 3: ADDING UNIQUE CONSTRAINTS (PREVENTION)")
    
    session = Session()
    try:
        # Define business key constraints for each table
        constraints = [
            {
                'table': 'raw_cooispi',
                'name': 'uq_cooispi_order_material',
                'columns': "(raw_data->>'Order', raw_data->>'Material')",
                'comment': 'Unique: Production Order + Material'
            },
            {
                'table': 'raw_mb51',
                'name': 'uq_mb51_doc_item_material_mvt',
                'columns': "(raw_data->>'Material Document', raw_data->>'Material Doc.Item', raw_data->>'Material', raw_data->>'Movement Type')",
                'comment': 'Unique: Material Doc + Item + Material + Movement Type'
            },
            {
                'table': 'raw_zrmm024',
                'name': 'uq_zrmm024_material_batch',
                'columns': "(raw_data->>'Material', raw_data->>'Batch')",
                'comment': 'Unique: Material + Batch'
            },
            {
                'table': 'raw_zrsd002',
                'name': 'uq_zrsd002_billing_doc_item',
                'columns': "(raw_data->>'Billing Document', raw_data->>'Billing Item')",
                'comment': 'Unique: Billing Document + Item'
            },
            {
                'table': 'raw_zrsd004',
                'name': 'uq_zrsd004_delivery_item',
                'columns': "(raw_data->>'Delivery', raw_data->>'Item')",
                'comment': 'Unique: Delivery + Item'
            },
            {
                'table': 'raw_zrsd006',
                'name': 'uq_zrsd006_sales_order_item',
                'columns': "(raw_data->>'Sales Document', raw_data->>'Item')",
                'comment': 'Unique: Sales Document + Item'
            },
            {
                'table': 'raw_zrfi005',
                'name': 'uq_zrfi005_customer_document',
                'columns': "(raw_data->>'Customer', raw_data->>'Document Number')",
                'comment': 'Unique: Customer + Document Number'
            },
        ]
        
        print("üîí Adding UNIQUE constraints to prevent duplicates:\n")
        
        for constraint in constraints:
            try:
                # Check if constraint already exists
                check_sql = text(f"""
                    SELECT 1 FROM pg_constraint 
                    WHERE conname = :name
                """)
                exists = session.execute(check_sql, {'name': constraint['name']}).fetchone()
                
                if exists:
                    print(f"  ‚ö† {constraint['table']}: Constraint already exists, skipping")
                    continue
                
                # Add constraint
                sql = text(f"""
                    ALTER TABLE {constraint['table']}
                    ADD CONSTRAINT {constraint['name']}
                    UNIQUE {constraint['columns']}
                """)
                
                session.execute(sql)
                session.commit()
                
                print(f"  ‚úì {constraint['table']}: {constraint['comment']}")
                
            except Exception as e:
                session.rollback()
                print(f"  ‚úó {constraint['table']}: Failed - {e}")
        
        # Also add indexes for better query performance
        print("\nüìá Adding indexes for performance:\n")
        
        indexes = [
            {'table': 'raw_zrsd002', 'name': 'idx_zrsd002_billing_doc', 'column': "(raw_data->>'Billing Document')"},
            {'table': 'raw_mb51', 'name': 'idx_mb51_material_doc', 'column': "(raw_data->>'Material Document')"},
            {'table': 'raw_cooispi', 'name': 'idx_cooispi_order', 'column': "(raw_data->>'Order')"},
        ]
        
        for idx in indexes:
            try:
                sql = text(f"""
                    CREATE INDEX IF NOT EXISTS {idx['name']}
                    ON {idx['table']} {idx['column']}
                """)
                session.execute(sql)
                session.commit()
                print(f"  ‚úì {idx['table']}: Index created")
            except Exception as e:
                session.rollback()
                print(f"  ‚úó {idx['table']}: {e}")
        
        print("\n‚úÖ Database constraints added - duplicates now BLOCKED at DB level!")
        
    except Exception as e:
        session.rollback()
        print(f"‚ùå Error: {e}")
        raise
    finally:
        session.close()

async def step4_create_validation_tests():
    """
    Step 4: Create automated data quality tests
    """
    print_section("STEP 4: CREATING AUTOMATED VALIDATION")
    
    validation_script = '''"""
Automated Data Quality Tests

Run this after every data load to ensure quality
"""

from sqlalchemy import create_engine, text
import os
from dotenv import load_dotenv
import pandas as pd
from pathlib import Path

load_dotenv()
engine = create_engine(os.getenv("DATABASE_URL"))

BASE_DIR = Path(__file__).parent
DEMODATA_DIR = BASE_DIR / "demodata"

def test_no_duplicates():
    """Test: No duplicates in raw tables"""
    print("\\nüîç TEST: Checking for duplicates in raw tables...")
    
    tests = [
        {
            'table': 'raw_zrsd002',
            'keys': "(raw_data->>'Billing Document', raw_data->>'Billing Item')"
        },
        {
            'table': 'raw_mb51',
            'keys': "(raw_data->>'Material Document', raw_data->>'Material Doc.Item')"
        },
        {
            'table': 'raw_cooispi',
            'keys': "(raw_data->>'Order', raw_data->>'Material')"
        },
    ]
    
    passed = True
    with engine.connect() as conn:
        for test in tests:
            result = conn.execute(text(f"""
                SELECT COUNT(*) - COUNT(DISTINCT {test['keys']}) as dups
                FROM {test['table']}
                WHERE {test['keys'].split(',')[0].strip()} IS NOT NULL
            """)).scalar()
            
            if result > 0:
                print(f"  ‚úó {test['table']}: {result} duplicates found!")
                passed = False
            else:
                print(f"  ‚úì {test['table']}: No duplicates")
    
    return passed

def test_row_counts_match_excel():
    """Test: Database row counts match Excel files"""
    print("\\nüîç TEST: Validating row counts against Excel...")
    
    files = {
        'zrsd002.xlsx': 'raw_zrsd002',
        'mb51.XLSX': 'raw_mb51',
        'cooispi.XLSX': 'raw_cooispi',
    }
    
    passed = True
    with engine.connect() as conn:
        for filename, table in files.items():
            excel_path = DEMODATA_DIR / filename
            if not excel_path.exists():
                continue
            
            df = pd.read_excel(excel_path)
            excel_count = len(df)
            
            db_count = conn.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
            
            diff_pct = abs(excel_count - db_count) / excel_count * 100 if excel_count > 0 else 0
            
            if diff_pct > 1:
                print(f"  ‚úó {table}: Excel {excel_count:,} vs DB {db_count:,} ({diff_pct:.1f}% diff)")
                passed = False
            else:
                print(f"  ‚úì {table}: {db_count:,} rows match Excel")
    
    return passed

def test_fact_billing_integrity():
    """Test: fact_billing has correct data"""
    print("\\nüîç TEST: Checking fact_billing integrity...")
    
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT 
                COUNT(*) as total_rows,
                COUNT(DISTINCT billing_document) as unique_docs,
                COUNT(CASE WHEN customer_name IS NULL THEN 1 END) as null_customers,
                SUM(net_value)/1000000000 as revenue_b
            FROM fact_billing
        """)).fetchone()
        
        passed = True
        
        # Should have data
        if result[0] == 0:
            print(f"  ‚úó No data in fact_billing!")
            passed = False
        else:
            print(f"  ‚úì Total rows: {result[0]:,}")
        
        # Should have customers
        if result[2] > 0:
            null_pct = result[2] / result[0] * 100
            print(f"  ‚úó {result[2]:,} rows ({null_pct:.1f}%) have NULL customer_name!")
            passed = False
        else:
            print(f"  ‚úì All rows have customer names")
        
        # Revenue should be reasonable
        if result[3] < 100 or result[3] > 1000:
            print(f"  ‚ö† Revenue {result[3]:.2f}B seems unusual")
        else:
            print(f"  ‚úì Revenue: {result[3]:.2f}B VND")
        
        return passed

def run_all_tests():
    """Run all data quality tests"""
    print("="*80)
    print("  AUTOMATED DATA QUALITY TESTS")
    print("="*80)
    
    results = {
        'No Duplicates': test_no_duplicates(),
        'Row Counts Match Excel': test_row_counts_match_excel(),
        'Fact Billing Integrity': test_fact_billing_integrity(),
    }
    
    print("\\n" + "="*80)
    print("  TEST RESULTS SUMMARY")
    print("="*80)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status}: {test_name}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("\\nüéâ ALL TESTS PASSED!")
    else:
        print("\\n‚ö†Ô∏è SOME TESTS FAILED - Please review!")
    
    return all_passed

if __name__ == "__main__":
    run_all_tests()
'''
    
    # Write validation script
    with open('automated_data_quality_tests.py', 'w', encoding='utf-8') as f:
        f.write(validation_script)
    
    print("‚úÖ Created: automated_data_quality_tests.py")
    print("   Run this after every data load to validate quality")

async def step5_update_loader_with_validation():
    """
    Step 5: Update loader to detect duplicates BEFORE loading
    """
    print_section("STEP 5: UPDATING LOADERS WITH PRE-LOAD VALIDATION")
    
    print("""
üìù Required updates to src/etl/loaders.py:

1. Add pre-load duplicate detection:
   - Check Excel file for internal duplicates
   - Warn if duplicates found
   - Optionally auto-deduplicate

2. Add post-load validation:
   - Compare row count: Excel vs Database
   - Fail if mismatch > 1%

3. Add better error reporting:
   - Log which rows failed and why
   - Show sample errors, not just count

These changes will be implemented in the loader code.
""")

async def step6_final_validation():
    """
    Step 6: Final comprehensive validation
    """
    print_section("STEP 6: FINAL COMPREHENSIVE VALIDATION")
    
    session = Session()
    try:
        print("üìä RAW TABLES:")
        raw_tables = ['raw_cooispi', 'raw_mb51', 'raw_zrmm024', 'raw_zrsd002',
                      'raw_zrsd004', 'raw_zrsd006', 'raw_zrfi005']
        
        for table in raw_tables:
            count = session.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
            print(f"  {table}: {count:,} rows")
        
        print("\nüìä FACT TABLES:")
        fact_tables = ['fact_production', 'fact_billing', 'fact_delivery',
                       'fact_ar_aging', 'fact_lead_time', 'fact_alerts']
        
        for table in fact_tables:
            count = session.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
            print(f"  {table}: {count:,} rows")
        
        # Check fact_billing specifically
        result = session.execute(text("""
            SELECT 
                COUNT(*) as rows,
                COUNT(DISTINCT billing_document) as docs,
                SUM(net_value)/1000000000 as revenue_b,
                COUNT(DISTINCT customer_name) as customers
            FROM fact_billing
        """)).fetchone()
        
        print(f"\nüí∞ FACT_BILLING DETAILS:")
        print(f"  Rows: {result[0]:,}")
        print(f"  Unique Documents: {result[1]:,}")
        print(f"  Revenue: {result[2]:.2f}B VND")
        print(f"  Unique Customers: {result[3]:,}")
        
        print("\n‚úÖ System ready for production!")
        
    finally:
        session.close()

async def main():
    """
    Execute comprehensive data quality fix & prevention system
    """
    print("="*80)
    print("  COMPREHENSIVE DATA QUALITY FIX & PREVENTION")
    print("="*80)
    print("\nThis will:")
    print("1. Re-load all raw data from Excel")
    print("2. Re-transform fact tables")
    print("3. Add UNIQUE constraints (prevent duplicates)")
    print("4. Create automated validation tests")
    print("5. Update loaders with validation")
    print("6. Final comprehensive validation")
    
    try:
        await step1_reload_raw_data()
        await step2_retransform_fact_tables()
        await step3_add_unique_constraints()
        await step4_create_validation_tests()
        await step5_update_loader_with_validation()
        await step6_final_validation()
        
        print("\n" + "="*80)
        print("  ‚úÖ COMPREHENSIVE FIX COMPLETED!")
        print("="*80)
        print("\nPrevention Mechanisms Now Active:")
        print("  ‚úì Database UNIQUE constraints block duplicates")
        print("  ‚úì Automated tests validate data quality")
        print("  ‚úì Loaders have pre/post-load validation")
        
    except Exception as e:
        print(f"\n‚ùå FAILED: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
